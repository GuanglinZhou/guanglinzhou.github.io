---
layout:     post
title:      "逻辑回归更新篇"
subtitle:   "逻辑回归 广义线性模型 指数分布簇 Softmax回归"
date:       2018-03-25 08:00:00
author:     "guanglinzhou"
header-img: "img/post-bg-2015.jpg"
tags:
    - 算法
    - ML
---

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

### 逻辑回归更新篇
@(机器学习经典算法总结)


----------
逻辑回归基本是我们所有人学习的第一个分类器，分类器从概率意义上理解，代表的是什么意思，以最简单的二分类来举例吧，假设样本数据

$$
D=(x^{(i)},y^{(i)}),\ i\ for\ 1,2,3,...\\
y^{(i)}\in(-1,+1)
$$
那么我们对于新的数据$$x^{(j)}$$肯定是要得到$$P(y=-1\ |\ x^{(j)})以及P(y=+1\ |\ x^{(j)})$$
哪个概率大，那么我们就采用该类别。
分类器所得到也即是$$P(c\ |\ x)$$
#### 生成式模型和判别式模型
目前分类模型根据获取$$P(c\ |\ x)$$的方式不同，分为生成时模型（generative models）和判别式模型（discriminative models）
[从CMU的教材来看](http://www.cs.cmu.edu/~epxing/Class/10701/slides/LR15.pdf)
![](https://ws1.sinaimg.cn/large/006tKfTcgy1fppgoiif1ej30dn05g74m.jpg)
从上图可看出，当使用生成式分类器时，我们依靠`所有的点`来学习生成模型。当使用判别式分类器时，我们主要关心`决策边界`。
那么这两种模型从概率角度怎么解释呢？
- 判别式模型：给定$$x$$，通过直接建模$$P(c\ |\ x)$$来预测$$c$$，常见模型为决策树，BP神经网络，SVM，当然还包括本节的LR；
- 生成式模型：通过联合概率求解$$P(c\ |\ x)=\frac{P(x,c)}{P(x)}$$，通过贝叶斯定理，可知$$P(c\ |\ x)=\frac{P(c)\ P(x\ |\ c)}{P(x)}$$，目前我了解的经典算法中，只有朴素贝叶斯；


----------
#### LR模型表达式和损失函数
那至此，我们也明确了，既然LR是判别式模型，就需要定义决策边界了。
经常把线性回归和逻辑回归关联到一起，虽然一个是回归模型，一个是分类模型，但是二者却有着千丝万缕的联系。
我们定义决策边界，可以考虑利用一个超平面，将正负样本分隔开，超平面我们可以使用线性模型去拟合。
那么此时，我们所要求的条件概率就是$$P(c\ |\ x,\omega),\omega表示线性回归的参数。$$
如图所示![](https://ws4.sinaimg.cn/large/006tNc79gy1fppznw7itrj30bm06x749.jpg)
分离面的方程为$$\vec{\omega}*\vec{x}=0$$

$$\begin{cases}
\omega*x>0 ,y=1\\
\omega*x<0,y=0
\end{cases}$$
使用sigmoid函数将线性回归>0的标记为正类，反之则负类。
那么
$$
P(Y=1\ |\ x,\omega)=\frac{1}{1+exp(-\omega*x)}=\frac{exp(\omega*x)}{1+exp(\omega*x)}\\
P(Y=0\ |\ x,\omega)=\frac{1}{1+exp(\omega*x)}
$$
令
$$h_\omega(x)=P(Y=1\ |\ x,\omega)$$
我们定义几率为正类的概率/负类的概率:

$$
\frac{P(Y=1\ |\ x,\omega)}{P(Y=0\ |\ x,\omega)}=exp(\omega * x)
$$
则对数几率即为
$$
log(\frac{P(Y=1\ |\ x,\omega)}{P(Y=0\ |\ x,\omega)})=\omega * x
$$
这也就是逻辑回归的由来，本质是对数几率回归。
则同线性回归一致，我们仍然使用最大似然估计去估计模型的参数$$\omega$$
这里最大似然估计的形式和**Bernoulli分布**的形式一致，实际在求解逻辑回归参数时，正是假设其样本服从伯努利分布，和线性回归的正态分布同理解得。
最大似然估计的原始形式为
$$
L(\omega)=\prod_{i=1}^{n}(\frac{exp(\omega*x^{(i)})}{1+exp(\omega*x^{(i)})})^{y^{(i)}}(\frac{1}{1+exp(\omega*x^{(i)})})^{1-y^{(i)}}
$$
取对数似然
$$
\begin{split}
LL(\omega)&=\sum_{i=1}^{n}\left(y^{(i)}*log(\frac{exp(\omega*x^{(i)})}{1+exp(\omega*x^{(i)})})+(1-y^{(i)})*log(\frac{1}{1+exp(\omega*x^{(i)})})\right)\\
 &= \sum_i\left(y^{(i)}\omega x^{(i)}+log(1+exp(\omega x^{(i)}))\right)
\end{split}
$$

$$
\frac{\partial LL(\omega)}{\omega_j}=\sum_{i}\left(x_j^{(i)}(y^{(i)}-\frac{exp(\omega x^{(i)})}{1+exp(\omega x^{(i)})})\right)=\sum_{i}\left(x_j^{(i)}\left(y^{(i)}-h_\omega(x^{(i)})\right)\right)
$$
可看出直接对偏导数置为0，无法求解$$\omega$$
逻辑回归的最大似然函数对于参数$$\omega$$来说是个凹函数，对似然函数取极大值，可以使用梯度上升来估计极大值。
而且，从最大似然函数出发可以得到逻辑回归的损失函数，即在似然函数前面加个`-1/m`。

$$
\begin{split}
J(\omega)&=-\frac{1}{m}\sum_{i}\left(y^{(i)}*log(h_\omega(x^{(i)}))+(1-y^{(i)})*log(1-h_\omega(x^{(i)}))\right)\\
\frac{\partial J(\omega)}{\omega_j}&=-\frac{1}{m}\sum_{i}\left(x_j^{(i)}\left(y^{(i)}-h_\omega(x^{(i)})\right)\right)
\end{split}
$$
我们需要求损失函数的最小值，相应的需要使用梯度下降法求解。
`可以看出，这里逻辑斯蒂回归的损失函数梯度表达式和我们在`[上篇博客](http://www.jameszhou.tech/2018/03/22/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/)`中对线性回归损失函数求的梯度表达式是一样的。
不同点是，逻辑斯蒂回归这里`$$h_{\theta}(x)是线性回归的h_{\theta}(x)求了个sigmod，这在编程时可以注意到。$$
#### Python实现逻辑回归
```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Date    : 2018-03-25 10:15:12
# @Author  : guanglinzhou (xdzgl812@163.com)
# @Link    : https://github.com/GuanglinZhou
# @Version : $$Id$$

'''
使用sklearn中make_moons数据，构建了个LR二分类模型
'''

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons


def sigmoid(x):
    return 1.0 / (1 + np.exp(-x))


def logistic(alpha, XMatrix, yMatrix, iterNum):
    m, n = np.shape(XMatrix)
    print(m)
    print(n)
    theta = np.ones((n, 1))
    loss_list = []
    for i in range(iterNum):
        hypothsis = np.squeeze(np.asarray(np.dot(XMatrix, theta)))
        hypothsis = sigmoid(hypothsis)
        loss = 0
        for j in range(m):
            if (yMatrix[j][0] == 1):
                loss += np.log(hypothsis[j])
            else:
                loss += np.log(1 - hypothsis[j])
        loss = -loss / m
        loss_list.append(loss)
        error = np.mat(hypothsis).transpose() - yMatrix
        theta = theta - alpha * np.dot(XMatrix.transpose(), error)
    return theta, loss_list


def plotLossFunction(loss_list, iterNum):
    fig = plt.figure()
    ax1 = fig.add_subplot(111)
    ax1.set_title('Loss Value')
    plt.xlabel('iter num')
    plt.ylabel('loss')
    plt.plot(list(range(iterNum)), loss_list, c='r')
    plt.show()


def plotLR(Xmat, ymat, thetaUpdate):
    Xarray = np.array(Xmat)
    yarray = np.array(ymat)
    col = {0: 'r', 1: 'b'}
    plt.figure()
    for i in range(Xarray.shape[0]):
        plt.plot(Xarray[i, 0], Xarray[i, 1], col[yarray[i][0]] + 'o')
    plt.ylim([-1.5, 1.5])
    plt.plot(Xarray[:, 0],
             (-(thetaUpdate[0][0] * Xarray[:, 0] + thetaUpdate[2][0] * Xarray[:, 2]) / thetaUpdate[1][0]).transpose(),
             c='g')
    plt.show()


if __name__ == '__main__':
    x, y = make_moons(250, noise=0.25)
    XarrayLess = x
    x = np.ones((XarrayLess.shape[0], XarrayLess.shape[1] + 1))
    x[:, :-1] = XarrayLess
    x = np.mat(x)
    y = np.mat(y).transpose()
    alpha = 0.01
    iterNum = 5000
    theta, loss_list = logistic(alpha, x, y, iterNum)
    plotLR(x, y, theta)
    plotLossFunction(loss_list, iterNum)
    

```

数据不是线性可分，下图的超平面参数是通过梯度下降法求得的。
![](https://ws4.sinaimg.cn/large/006tKfTcly1fpp5c4k1fbj30hs0dcmxm.jpg)

损失函数至随迭代次数的变化情况，梯度下降收敛了。
![](https://ws2.sinaimg.cn/large/006tKfTcly1fpp5cavop0j30hs0dcaa8.jpg)

----------
#### 广义线性模型和指数分布簇
曾经看到一个问题“为什么逻辑斯蒂回归使用`sigmoid`函数作为分线性映射，将y值映射到0-1之间，为什么不是别的非线性函数？”
学习完了广义线性模型就可以知道了。
**Here we go**
广义线性模型**GLM**是对线性模型的有效扩充，线性回归和逻辑回归都是广义线性模型的特殊形式。

广义线性模型条件：
- 给定样本$$x$$和参数$$\theta$$后，$$y$$的条件概率$$P(y\ | \ x;\theta)$$服从指数分布簇；
- 预测$$T(y)$$的期望，即计算$$E(T(y)\ | \ x)$$；
- $$\eta$$与$$x$$之间的线性的，即$$\eta=\theta x$$；

指数分布簇满足：

$$
P(y,\eta)=b(y)*exp(\eta T(y)-a(\eta))
$$
我们使用来看逻辑回归是否属于广义线性模型。
逻辑回归假设样本服从伯努利分布，正样本的概率为$$\phi$$
则
$$
P(y)=\phi^{y}*(1-\phi)^{1-y}
=exp(yln(\phi)+(1-\phi)(1-y))
=exp(y*ln\frac{\phi}{1-\phi}+ln(1-\phi)
$$
对比上面的指数分布簇，可知：

$$
b(y)=1;T(y)=y;\eta=ln\frac{\phi}{1-\phi};a(\eta)=-ln(1-\phi)
$$

有伯努利的性质可知：$$E(y\ |\ x)=\phi$$
所以
$$
E(T(y)\ | \ x)=E(y\ |\ x)=\phi
$$
由上面解得以及广义线性模型性质可知：

$$\phi=\frac{e^{\eta}}{1+e^{\eta}}=\frac{e^{\theta x}}{1+e^{\theta x}}=\frac{1}{1+e^{-\theta x}}$$
这就解出了`sigmoid`函数的形式了，说明为什么逻辑回归要使用sigmoid做非线性化，是由广义线性模型推出来的。
`这里线性回归的就不推导了，类比应该可以推出来；从我最后贴出来的链接也有推导过程。`
有了指数分布簇就可以推导出逻辑回归中的伯努利分布参数$$\phi$$和模型参数$$\theta$$的关系，线性回归也是同理，继而使用最大似然估计模型参数。
实际上，一般我们遇到的正态分布，伯努利分布，二项分布（n重伯努利分布），指数分布，泊松分布都是属于指数分布簇的，都可以使用广义线性模型推导。


----------
#### 将逻辑回归应用到多分类
如果想将逻辑回归应用于多分类中，两种方式：
1、机器学习中的将二分类模型组合成多分类模型：
- OvO:假设数据集中有m个样本，N个类别。One vs One是将多分类问题转换为$$\frac{N*(N-1)}{2}$$个二分类器，最终的类别由这$$\frac{N*(N-1)}{2}$$个二分类器投票决定，取预测的最多的类别。

| train     | label | 
| :--------: | :--------:|
| x1  | C1|
| x2  | C2|
| x3  | C3|
| x4  | C1|
| x5  | C2|
test
| test     | label | 
| :--------: | :--------:|
| x6  | |
如上的样本构造了3个二分类器，这三个分类器分别预测x6的类别，最后的label由三个分类器投票产生。
- OvR:One vs Rest将多分类器转换为N个二分类器，每次以其中一个类别样本作为正样本，其余类别样本作为负样本，训练N个二分类器。如果测试时仅有一个分类器预测为正类，则对应的类别标记即为输出，如果多个分类器预测为正类，取置信度更大的分类器类别标记。

两种方式进行比较，OvO每次使用两个类别的数据进行训练，得到了$$\frac{N*(N-1)}{2}$$个二分类器；OvR每次使用所有的数据进行训练，得到了N个二分类器。多数情况下，两者性能差不多。

2、将二项逻辑斯蒂回归扩展到多项逻辑斯蒂回归（Softmax回归）
[知乎—广义线性模型（Generalized Linear Model）](https://zhuanlan.zhihu.com/p/22876460)这篇博客利用广义线性模型对多项式分布，推出了Softmax回归的假设函数和损失函数。
以及[Stanford教材](http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92)
![](https://ws1.sinaimg.cn/large/006tNc79gy1fppzo25nmxj30eb03mt8r.jpg)
![](https://ws2.sinaimg.cn/large/006tNc79gy1fppzoo9qdlj30it07b0tf.jpg)
从上可看出当Softmax回归的类别减少到两个类别时，就变成了Logistic回归的形式了。

[Stanford教材最后](http://deeplearning.stanford.edu/wiki/index.php/Softmax_Regression)
教材的最后，还有个Softmax和K个逻辑回归二分类的比较，当类别之间是互斥时，适合选择Softmax多分类器，当类别不满足互斥时，更适合建立K个独立的logistic二分类器。

----------

项目地址：
[GitHub](https://github.com/GuanglinZhou/ML_Python/tree/master/LogisticRegressionExample)

参考博客：
- [知乎—机器学习系列-广义线性模型](https://zhuanlan.zhihu.com/p/24967776)
- [逻辑回归的前生今世](https://blog.csdn.net/cyh_24/article/details/50359055)
- [知乎—广义线性模型](https://zhuanlan.zhihu.com/p/22876460)
- [机器学习（二）广义线性模型：逻辑回归与Softmax分类](http://lib.csdn.net/article/machinelearning/37687)
