
---
layout:     post
title:      "TensorFlow笔记"
subtitle:   "《Tensorflow实战Google深度学习框架》"
date:       2018-03-07 12:00:00
author:     "guanglinzhou"
header-img: "img/post-bg-2015.jpg"
tags:
    - TensorFlow
---

#### Tensorflow实战Google深度学习框架笔记
@(Tensorflow)


----------
#### Chap3 TensorFlow入门
前两章主要讲的是深度学习简介和TensorFlow的安装事宜，直接从第三章开始笔记。

TensorFlow中的三个模型——计算模型（图Graph），数据模型（张量Tensor），运行模型（会话Session）
先简单介绍一下三个模型是什么，再记一下它们之间是如何协作的。
**计算模型（图Graph）**
首先，计算图Graph定义了一个“作用域”，不同图之间的张量和运算不会共享，可以通过`tf.Graph`函数来生成新的计算图。
TensorFlow中每个计算都是计算图上的一个节点，节点之间的边描述计算之间的依赖关系。（`计算图上每个节点都是一个运算`）

TensorFlow会把程序中定义的计算自动转化为计算图上的节点这种形式，系统会维护一个默认的计算图(`tf.get_default_graph()`)<也可以自己生成新的计算图。
**数据模型（张量Tensor）**
在TensorFlow中，所有数据都是通过张量的形式来表示，可以将tensor理解为多维数组：


- 零阶张量——标量；
- 一阶张量——向量；
- n阶张量——n维数组；

其实张量只是对计算结果的`引用`，本身并没有保存任何值。
```python
	import tensorflow as tf
    a = tf.constant([1, 2], dtype=tf.int8, name='a')
	b = tf.constant([3, 4], dtype=tf.int8, name='b')
	result = a + b
	print(result)
	# 结果为：Tensor("add:0", shape=(2,), dtype=int8)
```

但可以通过在会话Session中`tf.Session().run(tensor_name)`来得到张量引用的计算结果。
**运行模型（会话Session）**
TensorFlow程序分为两阶段，第一阶段是定义计算图中所有的计算，第二阶段是通过会话Session执行计算。
一般使用会话需要显示的调用会话生成函数和会话关闭函数，因为（会话管理TensorFlow程序运行时的所有资源，计算完成之后需要关闭会话来释放资源，如果会话没有关闭会出现资源泄漏的问题）。

    sess=tf.Session()
	sess.run(tensor_name)
	sess.close()

可以使用Python的上下文管理工具，避免因为忘记关闭会话或者因为异常情况导致关闭会话函数没有执行的情况。

    with tf.Session() as sess:
	    sess.run()


----------
了解了TensorFlow这三个模型，就可以愉快的开始编写TensorFlow程序了。
接着上文的，简单实现个相加代码。

    import tensorflow as tf
    a=tf.constant([1.0,2.0],name='a')
    b=tf.constant([3.0,4.0],name='b')
    result=a+b
    with tf.Session() as sess:
	    sess.run(result)
	    	   

----------
**TensorFlow实现三层全连接神经网络**

![](https://ws1.sinaimg.cn/large/006tKfTcgy1fp4b98wq27j30io0b8dii.jpg)

三层神经网络，输入层是个1* 2的矩阵$X$，输入层到隐藏层是个2* 3的矩阵$W_1$，隐藏层到输出层是个3*1的矩阵$W_2$，输出层得到一个实值。
输入层一般使用常量`tf.constant`来表示，参数矩阵$W$使用变量来表示`tf.Variable`
> TensorFlow 中最基本的单位是常量（Constant）、变量（Variable）和占位符（Placeholder）。常量定义后值和维度不可变，变量定义后值可变而维度不可变。在神经网络中，变量一般可作为储存权重和其他信息的矩阵，而常量可作为储存超参数或其他结构信息的变量。占位符（Placeholder）顾名思义，先占个位置，在运行时再通过feed_dict{}给这个占位符“喂”值。

**TensorFlow中训练神经网络过程：**

- 定义网络结构和前向传播的输出结果；
- 定义损失函数以及选择反向传播算法；
- 生成会话Session并且在训练数据上反复运行反向传播算法。


----------


#### Chap4 深层神经网络

维基百科对于深度学习的定义是：**一类通过多层非线性变换对高复杂性数据建模算法的合集**
深度学习有两个特点——多层和非线性。
多层即多个隐藏层；
非线性即对于线性模型使用激活函数(Activation function)做了个非线性变换。

![](https://ws3.sinaimg.cn/large/006tKfTcgy1fp4bd1751vj30dr0763z3.jpg)

TensorFlow中提供了7种不同的非线性函数，
`tf.nn.relu、tf.sigmoid、tf.tanh`比较常用。
本章中介绍的损失函数、学习率、过拟合、zheng'ze'hua和机器学习基本一致，就不做笔记了。


----------


#### Chap6 图像识别与卷积神经网络（Convolutional Neural Network）

卷积神经网络在图像分类数据集上有非常突出的表现，这是由其结构根本决定的，在前面章节学习到，一个网络的效果取决于网络的结构，损失函数定义，激活函数选取，是否有正则项等等，起决定作用的就是神经网络的结构。
前面章节学习的都是全连接神经网络（相邻两层之间的任意节点都是相连的）

![](https://ws1.sinaimg.cn/large/006tKfTcgy1fp4bdbcoejj308a054jsz.jpg)

而卷积神经网络的结构如下：

![](https://ws3.sinaimg.cn/large/006tKfTcgy1fp4bdiuv8pj309j04jt9d.jpg)

从后面学习的循环神经网络我们可以看出网络结构又发生了翻天覆地的变化，因为具体问题具体分析。从全连接到卷积的转变，还是由于全连接不适用于图像分类或者图像领域。
**全连接神经网络处理图像的最大问题在于全连接层的参数太多了**
比如说，对于手写数字识别数据集MNIST来说，每张图片的大小为$28*28*1$，我们使用三层的全连接神经网络，设置隐层的神经元节点为500个，输入层和输出层节点数可知分别为$28* 28和10$。
则，输入层到隐层的参数个数为$28* 28*500+500$，隐层到输出层的参数个数为$500 *10+10$，总计为397510个参数。
可以看到对于不算太大的数据集，不是很复杂的网络结构就需要估计几十万个参数。
参数过多无疑会减慢计算速度而且还容易过拟合问题。
所以卷积神经网络应运而生。

卷积神经网络和全连接神经网络不同的地方在于多了**卷积层和池化层**。

![](https://ws2.sinaimg.cn/large/006tKfTcgy1fp4bdrv5k5j30hu066gno.jpg)

从前面分析的全连接网络的缺陷可以看出，因为由于图像这种数据导致输入层的维数很大，我们只是使用一个正常大小的网络去预测分类结果，就需要估计非常多的网络参数。所以在保证不丢失输入信息的前提下，需要对输入数据降维，提取主要特征。
**卷积层：**卷积层中每个节点的输入只是上一层神经网络的一小块，卷积层试图对神经网络中的每一小块进行更加深入地分析得到抽象程度更高的特征。**一般来说，通过卷积层处理过的节点矩阵会变得更深。**


----------


`原书中所讲的过滤器或者卷积核的输出是个单位节点矩阵的说法和我的理解以及所查的资料不太符合，很可能是我的理解不对，所以就不按书中记录，按照经典的LeNet模型记录。`


----------

卷积层利用过滤器或称为卷积核（一个小方阵数据一般为3* 3或5* 5）去学习图像特征。
假如输入数据表示为5* 5的矩阵，使用3* 3的卷积核：

![](https://ws4.sinaimg.cn/large/006tKfTcgy1fp4bets1i0j30ah05zmza.jpg)

卷积核在输入数据从左到右，从上到下移动（会有个步长参数可供调节，假如这里的步长为1），每移动一次进行卷积操作，即对应元素相乘并求和。最终得到一个3* 3的矩阵，即卷积特征。

![](https://ws4.sinaimg.cn/large/006tKfTcgy1fp4bf1rmt5j30dh09lq4u.jpg)
我们可以知道，当卷积核中（矩阵中）的数值变化时，得到的卷积特征也是不一样的，则可以通过变换卷积核来挖掘到不同的特征。

![](https://ws3.sinaimg.cn/large/006tKfTcgy1fp4bfcn78oj30iq0lun4e.jpg)

比如我们使用5个卷积核对原始5* 5的图像进行卷积，就把原来5* 5的数据转换为了3 * 3* 5的特征了，那么这么做有什么好处呢？
从前面描述可知，当使用全连接神经网络来处理图像数据时，网络的参数是非常多的，多的难以进行训练，即使训练好了也会有过拟合的风险。
对5* 5的图像，使用500个隐层，参数个数为5* 5* 500+500=13000个参数，如果是5个3* 3的卷积核，需要3* 3* 5+5=50个参数。
真的是`巨幅`减少了神经网络中参数。
从上面计算可以发现，卷积层参数个数`只与卷积核的尺寸和深度，以及当前层的深度相关`。
我举例的
- 卷积核尺寸为3*3；
- 卷积核的深度为5，即使用5个卷积核；
- 当前层即输入层深度为1，我举例的是个黑白图像，深度为1，如果是RGB图像，深度为3；

**池化层：**池化层的作用在于进一步缩小矩阵的尺寸，从而减小最后全连接层的参数。
池化层使用的类似卷积核的结构，不过不是使用卷积操作而是使用简单的最大值或者均值处理。
如下面的就是个最大池化层。

![](https://ws3.sinaimg.cn/large/006tKfTcgy1fp4bfwpafbj309x048gmd.jpg)

----------
