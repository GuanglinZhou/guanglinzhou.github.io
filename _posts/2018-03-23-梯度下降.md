---
layout:     post
title:      "梯度下降"
subtitle:   “原来一直都不算懂了梯度下降"
date:       2018-03-22 08:00:00
author:     "guanglinzhou"
header-img: "img/post-bg-2015.jpg"
tags:
    - ML
---

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>




----------
一直以为自己懂了梯度下降法，直到编程实现一遍，发现有些概念其实理解的并不清晰。
这篇Blog旨在：
- 梯度下降法的推导；
- 常用的几种梯度下降法并编程实现；
- 使用梯度下降法求解线性模型参数的例子；



----------
[数学回顾笔记（一）——方向导数和梯度](http://blog.leanote.com/post/hidamari/%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E6%96%B9%E5%90%91%E5%AF%BC%E6%95%B0%E5%92%8C%E6%A2%AF%E5%BA%A6)回顾了方向导数和梯度的概念，明确了，梯度方向为函数增长最快的方向，负梯度方向为函数减小最快的方向。
梯度下降法是求解无约束最优化问题的常用方法，基于泰勒展开式推出的，

$$
f(x)=f(x_0)+(x-x_0)f(x_0)^{(1)}+\frac{(x-x_0)^2}{2!}f(x_0)^{(2)}+...+\frac{(x-x_0)^n}{n!}f(x_0)^{(n)}+o(n)
$$

根据泰勒展开式可以无限逼近原表达式。
则，来看梯度下降法。
梯度下降法使用的是一阶泰勒展开式（忽略二阶及以上的高阶无穷小）

$$
f(x)=f(x_0)+(x-x_0)f(x_0)^{(1)}
$$

但是这里的自变量是向量，则把自变量x变为向量$\vec{x}$，将一阶偏微分变为梯度，转换为下式：

$$
f(\vec{x^k}+\alpha\vec{d})=f(\vec{x^k})+\alpha g_k^T\vec{d}\\
g_k^T=g(x^{(k)})=\nabla f(x^{(k)})表示f(x)在x^{(k)}的梯度，是个向量$$

我们的目的是使得上式去最小，则向量$g_k^T和\vec{d}$的方向应相反，即$\vec{x^{(k+1)}}$应该是$\vec{x^{k}}$沿着负梯度方向，更新的大小为$\alpha$。
上述就是梯度下降法的思想了，非常简洁明了对不对，唯一的参数就是更新的步长(学习率)$\alpha$了。


----------
#### 线性模型使用梯度下降法的公式推导。
在我之前的博文[James Zhou Blog—线性回归](http://www.jameszhou.tech/2018/03/19/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AC%94%E8%AE%B0/)中讲到，通过最大似然法估计线性回归的参数得到了最小二乘的形式，即使用均方误差来作为损失函数。

$$
J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}{(\ f(x^{(i)})-y^{(i)}\ )^2}
$$

梯度下降法，通过迭代的方式，使得参数$\theta$沿着损失函数下降最快的方向，每次更新$\alpha$步。
则：

$$
\frac{\partial{J(\theta)}}{\partial{\theta}}=\frac{1}{m}\sum_{i=1}^{m}{(\ f(x^{(i)})-y^{(i)}\ )*x^{(i)}}
$$

上述参数$\theta$是个向量，这点在程序中也会显现，对矩阵和向量不是很清楚，这里实现可能会有点问题，我自己也试了挺久的，建议自己实现一遍，这样会更清晰。

$$
\theta=\theta-\alpha*\frac{\partial{J(\theta)}}{\partial{\theta}}
$$

到此梯度下降法的推导就结束了。


----------
#### 常用的几种梯度下降法并编程实现

上述的梯度下降法推导实际是批梯度下降法，即**BatchGradientDescent**
可以看到，每次参数$\theta$的更新都需要使用所有的训练数据：对所有数据计算出`loss`，并对参数$\theta$求梯度，这样的方式，在样本量比较大的时候，迭代速度非常慢。
伪代码如下：
![](https://ws1.sinaimg.cn/large/006tNc79gy1fpmqg1d271j30eg07dt9m.jpg)
所以出现了随机梯度下降法，即**StochasticGradientDescent**，每次随机选择一个样本，使用其`loss`来更新参数$\theta$。
伪代码如下：
![](https://ws2.sinaimg.cn/large/006tNc79gy1fpmq4z665rj309k07h756.jpg)
随机梯度下降，迭代速度很快，但是毕竟不是使用所有样本的loss来求梯度，有的迭代轮次中，不是向着整体最优化的方向。但是时间和性能折中，用随机梯度还是比较多的。
小批量梯度下降法（MiniBatchGradientDescent）是批梯度和随机梯度的结合，每次选择一个Batch大小的样本来求梯度进而更新参数。
代码实现：
```python
def batchGradientDescent(alpha, XMatrix, yMatrix, iterNum):
    m, n = np.shape(XMatrix)
    theta = np.ones((n, 1))
    for i in range(iterNum):
        error = yMatrix - np.dot(XMatrix, theta)
        theta = theta + alpha * np.dot(error.transpose(), XMatrix).transpose() / m
    return theta


def stochasticGradientDescent(alpha, XMatrix, yMatrix, iterNum):
    m, n = np.shape(XMatrix)
    theta = np.ones((n, 1))
    randomNum = random.randint(0, m - 1)
    for i in range(iterNum):
        error = yMatrix[randomNum] - np.dot(XMatrix[randomNum], theta)
        theta = theta + alpha * (error * XMatrix[randomNum]).transpose()
    return theta


def miniBatchGradientDescent(alpha, XMatrix, yMatrix, iterNum, batch):
    m, n = np.shape(XMatrix)
    theta = np.ones((n, 1))
    indexList = list(range(m))
    slice = random.sample(indexList, batch)
    XMatrix = XMatrix[slice]
    yMatrix = yMatrix[slice]
    for i in range(iterNum):
        error = yMatrix - np.dot(XMatrix, theta)
        theta = theta + alpha * (error.transpose() * XMatrix).transpose()
    return theta

```


----------
#### 使用梯度下降法求解线性模型参数的例子


批梯度拟合图：
![](https://ws2.sinaimg.cn/large/006tNc79gy1fpmq7gmpz7j30hs0dcmxf.jpg)

随机梯度拟合图：
![](https://ws2.sinaimg.cn/large/006tNc79gy1fpmq63sqjnj30hs0dcwer.jpg)

mini-Batch梯度下降拟合图：
![](https://ws2.sinaimg.cn/large/006tNc79gy1fpmq5oxemtj30hs0dcdg3.jpg)

梯度下降未收敛图：
![](https://ws1.sinaimg.cn/large/006tNc79gy1fpmq5hbavfj30hs0dcjrk.jpg)
当`学习率选取的比较大的时候，很可能会出现梯度下降法无法收敛的情况，损失函数值会越来越大。这点要注意！这个错误我一开始找了很久，以为自己的代码有问题，后来发觉是学习率不合适。`
损失函数的值随着迭代次数：
![](https://ws1.sinaimg.cn/large/006tNc79gy1fpmq5aqlidj305c04ldfr.jpg)


----------
代码和数据地址：[GitHub——GradientDescentExample](https://github.com/GuanglinZhou/ML_Python/tree/master/GradientDescentExample)；

参考博文：
- [详解梯度下降法的三种形式BGD、SGD以及MBGD](https://zhuanlan.zhihu.com/p/25765735)
- [梯度下降算法以及其Python实现](https://www.jianshu.com/p/9bf3017e2487)
- [梯度下降实用技巧II之学习率](https://blog.csdn.net/u012328159/article/details/51030961)
- [梯度下降算法步长和收敛条件的设置的一些看法](https://blog.csdn.net/silence1214/article/details/8854719)
- [随机梯度下降 批量梯度下降 确定训练模型的数据规模 判断梯度下降是否收敛](https://blog.csdn.net/lujiandong1/article/details/44960545)
- [梯度下降的原理（泰勒证明）](https://blog.csdn.net/niuniuyuh/article/details/76862019)